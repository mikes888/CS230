{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIf32DBJXzWc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import VarianceThreshold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "HXU0NYCyYIft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Processing"
      ],
      "metadata": {
        "id": "9y-G60G2YKuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract transaction data\n",
        "num_chunks = 10000\n",
        "chunks = []\n",
        "for chunk in pd.read_csv(\"/content/drive/My Drive/CS230_Folder/train_transaction.csv\",\n",
        "                         chunksize=num_chunks, engine='python'):\n",
        "    chunks.append(chunk)\n",
        "train_trans_df = pd.concat(chunks)"
      ],
      "metadata": {
        "id": "xJiaGZLbYMSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract identity data\n",
        "num_chunks = 10000\n",
        "chunks = []\n",
        "for chunk in pd.read_csv(\"/content/drive/My Drive/CS230_Folder/train_identity.csv\",\n",
        "                         chunksize=num_chunks, engine='python'):\n",
        "    chunks.append(chunk)\n",
        "train_identity_df = pd.concat(chunks)"
      ],
      "metadata": {
        "id": "zOz6p-ULYNSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge transaction and identity datasets\n",
        "whole_df = train_trans_df.merge(train_identity_df, on='TransactionID')"
      ],
      "metadata": {
        "id": "Joz4w2uTYOgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop features with too many null values\n",
        "minimum = 0.5\n",
        "null_percentage = whole_df.isnull().mean()\n",
        "whole_df = whole_df.drop(columns=null_percentage[null_percentage > minimum].index)"
      ],
      "metadata": {
        "id": "qWyaOyYZYPzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and encode categorical features\n",
        "trans_category_features = ['isFraud', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n",
        "    'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain',\n",
        "    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']\n",
        "\n",
        "identity_category_features = [f'id_{i}' for i in range(12,39)] + ['DeviceType', 'DeviceInfo']\n",
        "\n",
        "categories = trans_category_features + identity_category_features\n",
        "\n",
        "for feature in categories:\n",
        "  if feature in whole_df.columns:\n",
        "    le = LabelEncoder()\n",
        "    whole_df[feature] = le.fit_transform(whole_df[feature].astype(str))\n",
        "\n",
        "# Fill null values with -999\n",
        "whole_df.fillna(-999)"
      ],
      "metadata": {
        "id": "2kH6HCF2YQ55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract numerical features\n",
        "numerical_features = [feature for feature in whole_df.columns if feature not in categories]"
      ],
      "metadata": {
        "id": "uYxOx6PSYSDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "whole_df[numerical_features] = scaler.fit_transform(whole_df[numerical_features])"
      ],
      "metadata": {
        "id": "4NzfF5HCYUMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove low variance features\n",
        "selector = VarianceThreshold(threshold=0.05)\n",
        "matrix_whole_df = selector.fit_transform(whole_df)\n",
        "columns_kept = whole_df.columns[selector.get_support()]\n",
        "whole_df = pd.DataFrame(matrix_whole_df, columns=columns_kept)"
      ],
      "metadata": {
        "id": "lLXam38TYVik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get X and Y\n",
        "X = whole_df.drop('isFraud', axis=1)\n",
        "y = whole_df['isFraud']"
      ],
      "metadata": {
        "id": "kNE9WHtOYyFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get train, validation, and test sets\n",
        "X_train, X_test_valid, y_train, y_test_valid = train_test_split(X, y, test_size=0.02, random_state=20)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test_valid, y_test_valid, test_size=0.5, random_state=20)"
      ],
      "metadata": {
        "id": "LHEJ9AC7Y03d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any rows with null values\n",
        "X_train_dropped = X_train.dropna()\n",
        "X_valid_dropped = X_valid.dropna()\n",
        "X_test_dropped = X_test.dropna()\n",
        "y_train_dropped = y_train[X_train_dropped.index]\n",
        "y_valid = y_valid[X_valid_dropped.index]\n",
        "y_test = y_test[X_test_dropped.index]"
      ],
      "metadata": {
        "id": "Ddgrtw5SY2Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resampling for unbalanced dataset\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(random_state=20)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_dropped, y_train_dropped)"
      ],
      "metadata": {
        "id": "QB_W65V4Y4KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "F0Dswc8BY40p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class log_reg_model(tf.keras.Model):\n",
        "  def __init__(self, input_size):\n",
        "    super(log_reg_model, self).__init__()\n",
        "    self.linear = tf.keras.layers.Dense(1, kernel_initializer=tf.keras.initializers.GlorotNormal())\n",
        "\n",
        "  def call(self, x):\n",
        "    z = self.linear(x)\n",
        "    return z"
      ],
      "metadata": {
        "id": "T_F2Qb6ZY8N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training\n",
        "input_dim = X_train_resampled.shape[1]\n",
        "model_log_reg = log_reg_model(input_dim)\n",
        "\n",
        "criterion = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "X_train_tensor = tf.convert_to_tensor(X_train_resampled.values, dtype=tf.float32)\n",
        "y_train_tensor = tf.convert_to_tensor(y_train_resampled.values, dtype=tf.float32)\n",
        "\n",
        "num_epochs = 4000\n",
        "for epoch in range(num_epochs):\n",
        "  with tf.GradientTape() as tape:\n",
        "    outputs = model_log_reg(X_train_tensor)\n",
        "    loss = criterion(y_train_tensor, outputs)\n",
        "\n",
        "    gradients = tape.gradient(loss, model_log_reg.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model_log_reg.trainable_variables))\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.numpy():.4f}')"
      ],
      "metadata": {
        "id": "LPNrap45Y-JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation on Validation Set to test learning rate\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_curve, auc\n",
        "\n",
        "X_valid_tensor = tf.convert_to_tensor(X_valid_dropped.values, dtype=tf.float32)\n",
        "y_valid_tensor = tf.convert_to_tensor(y_valid.values, dtype=tf.float32)\n",
        "\n",
        "y_valid_tensor = model_log_reg(X_valid_tensor)\n",
        "\n",
        "y_pred = tf.math.sigmoid(y_valid_tensor)\n",
        "\n",
        "threshold = 0.5\n",
        "predicted = tf.squeeze(tf.cast(y_pred > threshold, tf.float32))\n",
        "\n",
        "y_valid_tensor = tf.squeeze(y_valid_tensor)\n",
        "\n",
        "correct = tf.equal(predicted, y_valid_tensor)\n",
        "accuracy = tf.reduce_mean(tf.cast((correct), tf.float32)).numpy()\n",
        "print(f'accuracy: {accuracy}')\n",
        "\n",
        "# Calculate AUC\n",
        "probs_np = y_pred.numpy().squeeze()\n",
        "y_valid_np = y_valid_tensor.numpy()\n",
        "auc = roc_auc_score(y_valid_np, probs_np)\n",
        "print(f'AUC: {auc}')\n",
        "\n",
        "# Confusion matrix\n",
        "confuse_matrix = confusion_matrix(y_valid_np, predicted.numpy())\n",
        "print(f'Confusion Matrix : {confuse_matrix}')\n",
        "\n",
        "# F1\n",
        "precision = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[0,1])\n",
        "recall = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[1,0])\n",
        "f1 = (2*precision*recall)/(precision + recall)\n",
        "print(f'Precision:{precision}')\n",
        "print(f'Recall:{recall}')\n",
        "print(f'F1: {f1}')"
      ],
      "metadata": {
        "id": "ES6SzQtAZ0wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating logistic regression model on the test set\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_curve, auc\n",
        "\n",
        "X_test_tensor = tf.convert_to_tensor(X_test_dropped.values, dtype=tf.float32)\n",
        "y_test_tensor = tf.convert_to_tensor(y_test.values, dtype=tf.float32)\n",
        "\n",
        "y_pred_tensor = model_log_reg(X_test_tensor)\n",
        "y_pred_train_tensor = model_log_reg(X_train_tensor)\n",
        "\n",
        "y_pred = tf.math.sigmoid(y_pred_tensor)\n",
        "y_pred_train = tf.math.sigmoid(y_pred_train_tensor)\n",
        "\n",
        "threshold = 0.5\n",
        "predicted = tf.squeeze(tf.cast(y_pred > threshold, tf.float32))\n",
        "\n",
        "y_test_tensor = tf.squeeze(y_test_tensor)\n",
        "\n",
        "correct = tf.equal(predicted, y_test_tensor)\n",
        "accuracy = tf.reduce_mean(tf.cast((correct), tf.float32)).numpy()\n",
        "print(f'accuracy: {accuracy}')\n",
        "\n",
        "# Calculate AUC\n",
        "probs_np = y_pred.numpy().squeeze()\n",
        "y_test_np = y_test_tensor.numpy()\n",
        "auc = roc_auc_score(y_test_np, probs_np)\n",
        "train_auc = roc_auc_score(y_train_tensor.numpy(), y_pred_train.numpy())\n",
        "print(f'AUC: {auc}')\n",
        "print(f'Training AUC: {train_auc}')\n",
        "\n",
        "# Confusion matrix\n",
        "confuse_matrix = confusion_matrix(y_test_np, predicted.numpy())\n",
        "print(f'Confusion Matrix : {confuse_matrix}')\n",
        "\n",
        "# F1\n",
        "precision = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[0,1])\n",
        "recall = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[1,0])\n",
        "f1 = (2*precision*recall)/(precision + recall)\n",
        "print(f'Precision:{precision}')\n",
        "print(f'Recall:{recall}')\n",
        "print(f'F1: {f1}')"
      ],
      "metadata": {
        "id": "TrtyOBBQZAdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "false_positive, true_positive, _ = roc_curve(y_test_np, probs_np)\n",
        "\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.plot(false_positive, true_positive, color='darkorange', lw=2, label='ROC curve')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IYYA2Ay4ZKeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "5USsNcRnZON1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and training XGB Model of Depth 10\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "eval_list = [(X_train_resampled, y_train_resampled), (X_valid_dropped, y_valid)]\n",
        "\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    scale_pos_weight= np.sum(y_train_resampled == 0)/np.sum(y_train_resampled == 1),\n",
        "    max_depth = 10,\n",
        "    n_estimators=500,\n",
        "    learning_rate= 0.00001)\n",
        "\n",
        "xgb_model.fit(X_train_resampled, y_train_resampled, eval_set = eval_list)\n",
        "\n",
        "results = xgb_model.evals_result()"
      ],
      "metadata": {
        "id": "lBxuKFh-ZQpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Binary Cross Entropy Loss\n",
        "epochs = range(len(results['validation_0']['logloss']))\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.plot(epochs, results['validation_0']['logloss'], label='Training')\n",
        "plt.plot(epochs, results['validation_1']['logloss'], label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Log Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZhxfScDtZfP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating XGB model of depth 10 on the test set\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "y_pred_xgb = xgb_model.predict(X_test_dropped)\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "roc_xgb = roc_auc_score(y_test, xgb_model.predict_proba(X_test_dropped)[:,1])\n",
        "roc_train = roc_auc_score(y_train_resampled, xgb_model.predict_proba(X_train_resampled)[:,1])\n",
        "confuse_matrix = confusion_matrix(y_test, y_pred_xgb)\n",
        "\n",
        "# Confuse matrix: [[TN, FP] [FN, TP]]\n",
        "print(f\"accuracy xgb depth 10: {accuracy_xgb}\")\n",
        "print(f\"roc_xgb depth 10: {roc_xgb}\")\n",
        "print(f\"roc_xgb training depth 10: {roc_train}\")\n",
        "print(f\"confuse_matrix depth 10: {confuse_matrix}\")\n",
        "\n",
        "# F1\n",
        "precision = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[0,1])\n",
        "recall = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[1,0])\n",
        "f1 = (2*precision*recall)/(precision + recall)\n",
        "print(f'Precision_depth10:{precision}')\n",
        "print(f'Recall_depth10:{recall}')\n",
        "print(f'F1_depth10: {f1}')"
      ],
      "metadata": {
        "id": "umfyFwLNZj4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter search for XGBoost\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "params = {\n",
        "    'learning_rate': np.logspace(np.log10(0.00001), np.log10(0.1), num=5),\n",
        "    'max_depth': [20, 25, 30],\n",
        "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.3,0.4,0.5,0.6,0.7, 0.8, 0.9],\n",
        "    'min_child_weight': [1, 2, 3],\n",
        "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
        "    'reg_alpha': [0, 0.1, 0.2, 0.3],\n",
        "    'reg_lambda': [0, 0.1, 0.2, 0.3],\n",
        "    'learning_rate': [0.01, 0.001, 0.0001, 0.00001]\n",
        "}\n",
        "\n",
        "param_search = RandomizedSearchCV(estimator=xgb_model, param_distributions = params, scoring='roc_auc', cv=5, n_iter = 30, n_jobs=-1)\n",
        "param_search.fit(X_valid_dropped, y_valid)\n",
        "\n",
        "best_params = param_search.best_params_\n",
        "best_model = param_search.best_estimator_\n",
        "print(best_params)"
      ],
      "metadata": {
        "id": "eruIHX0-aYIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimized XGBoost Model following hyperparameter optimization\n",
        "\n",
        "import xgboost as xgb\n",
        "\n",
        "eval_list = [(X_train_resampled, y_train_resampled), (X_valid_dropped, y_valid)]\n",
        "\n",
        "xgb_model_depth25 = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    scale_pos_weight= np.sum(y_train_resampled == 0)/np.sum(y_train_resampled == 1),\n",
        "    max_depth = 25,\n",
        "    subsample = 0.9,\n",
        "    min_child_weight = 1,\n",
        "    colsample_bytree = 0.5,\n",
        "    gamma = 0,\n",
        "    reg_lambda = 0.2,\n",
        "    reg_alpha = 0.2,\n",
        "    learning_rate= 0.00001,\n",
        "    n_estimators=500,\n",
        "    early_stopping_rounds=100)\n",
        "\n",
        "xgb_model_depth25.fit(X_train_resampled, y_train_resampled, eval_set=eval_list)\n",
        "\n",
        "results = xgb_model_depth25.evals_result()"
      ],
      "metadata": {
        "id": "NyMb32keaf7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(len(results['validation_0']['logloss']))\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.plot(epochs, results['validation_0']['logloss'], label='Training')\n",
        "plt.plot(epochs, results['validation_1']['logloss'], label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Log Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5LwHLR1janTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating optimized XGBoost Model on the test set\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "y_pred_xgb = xgb_model_depth25.predict(X_test_dropped)\n",
        "\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "roc_xgb = roc_auc_score(y_test, xgb_model_depth25.predict_proba(X_test_dropped)[:,1])\n",
        "train_roc = roc_auc_score(y_train_resampled, xgb_model_depth25.predict_proba(X_test_dropped)[:,1])\n",
        "confuse_matrix = confusion_matrix(y_test, y_pred_xgb)\n",
        "\n",
        "# Confuse matrix: [[TN, FP] [FN, TP]]\n",
        "print(f\"accuracy xgb depth 25: {accuracy_xgb}\")\n",
        "print(f\"roc_xgb depth 25: {roc_xgb}\")\n",
        "print(f\"Training roc depth 25: {train_roc}\")\n",
        "print(f\"confuse_matrix depth 25: {confuse_matrix}\")\n",
        "\n",
        "# F1\n",
        "precision = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[0,1])\n",
        "recall = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[1,0])\n",
        "f1 = (2*precision*recall)/(precision + recall)\n",
        "print(f'Precision_depth 25:{precision}')\n",
        "print(f'Recall_depth 25:{recall}')\n",
        "print(f'F1_depth25: {f1}')"
      ],
      "metadata": {
        "id": "GJcQLXrWazSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Threshold tuning for XGBoost on Validation Set\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "y_pred_valid_thres = xgb_model_depth25.predict_proba(X_valid_dropped)[:,1]\n",
        "\n",
        "threshold = 0.499\n",
        "y_pred_xgb = (y_pred_valid_thres > threshold).astype(int)\n",
        "\n",
        "accuracy_xgb = accuracy_score(y_valid, y_pred_xgb)\n",
        "roc_xgb = roc_auc_score(y_valid, y_pred_valid_thres)\n",
        "confuse_matrix = confusion_matrix(y_valid, y_pred_xgb)\n",
        "\n",
        "# Confuse matrix: [[TN, FP] [FN, TP]]\n",
        "print(f\"accuracy xgb depth 25: {accuracy_xgb}\")\n",
        "print(f\"roc_xgb depth 25: {roc_xgb}\")\n",
        "print(f\"confuse_matrix depth 25: {confuse_matrix}\")\n",
        "\n",
        "# F1\n",
        "precision = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[0,1])\n",
        "recall = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[1,0])\n",
        "f1 = (2*precision*recall)/(precision + recall)\n",
        "print(f'Precision_depth25:{precision}')\n",
        "print(f'Recall_depth25:{recall}')\n",
        "print(f'F1_depth25: {f1}')"
      ],
      "metadata": {
        "id": "7luOSV8Aa3of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of tuned XGBoost on Test Set\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "y_pred_test_thres = xgb_model_depth25.predict_proba(X_test_dropped)[:,1]\n",
        "\n",
        "threshold = 0.499\n",
        "y_pred_xgb = (y_pred_test_thres > threshold).astype(int)\n",
        "\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "roc_xgb = roc_auc_score(y_test, y_pred_test_thres)\n",
        "confuse_matrix = confusion_matrix(y_test, y_pred_xgb)\n",
        "\n",
        "# Confuse matrix: [[TN, FP] [FN, TP]]\n",
        "print(f\"accuracy xgb depth 25: {accuracy_xgb}\")\n",
        "print(f\"roc_xgb depth 25: {roc_xgb}\")\n",
        "print(f\"confuse_matrix depth 25: {confuse_matrix}\")\n",
        "\n",
        "# F1\n",
        "precision = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[0,1])\n",
        "recall = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[1,0])\n",
        "f1 = (2*precision*recall)/(precision + recall)\n",
        "print(f'Precision_depth25:{precision}')\n",
        "print(f'Recall_depth25:{recall}')\n",
        "print(f'F1_depth25: {f1}')"
      ],
      "metadata": {
        "id": "mRHmBRH0a-2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "false_positive, true_positive, _ = roc_curve(y_test, y_pred_valid_thres)\n",
        "\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.plot(false_positive, true_positive, color='darkorange', lw=2, label='ROC curve')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tSuyvhEy2C8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Networks"
      ],
      "metadata": {
        "id": "aYNyJMZmbWtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and Training a single-layer RNN Model with ReLU activation in Hidden Layers\n",
        "import tensorflow as tf\n",
        "\n",
        "model_relu = tf.keras.Sequential([tf.keras.layers.SimpleRNN(units=128, activation='relu', input_shape=(1, X_train_resampled.shape[1])),\n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model_relu.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
        "\n",
        "X_train_resampled_rnn = X_train_resampled.values.reshape((X_train_resampled.shape[0], 1, X_train_resampled.shape[1]))\n",
        "X_train_resampled_rnn = tf.convert_to_tensor(X_train_resampled_rnn, dtype=tf.float32)\n",
        "y_train_resampled_rnn = tf.convert_to_tensor(y_train_resampled.values, dtype=tf.float32)\n",
        "\n",
        "X_val_resampled_rnn = X_valid_dropped.values.reshape((X_valid_dropped.shape[0], 1, X_valid_dropped.shape[1]))\n",
        "X_val_resampled_rnn = tf.convert_to_tensor(X_val_resampled_rnn, dtype=tf.float32)\n",
        "y_val_resampled_rnn = tf.convert_to_tensor(y_valid.values, dtype=tf.float32)\n",
        "\n",
        "eval_set = (X_val_resampled_rnn, y_val_resampled_rnn)\n",
        "\n",
        "history = model_relu.fit(X_train_resampled_rnn, y_train_resampled_rnn, epochs=15, batch_size=32, validation_data = eval_set)"
      ],
      "metadata": {
        "id": "_SR3pd1F2F5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot binary cross entropy loss\n",
        "plt.plot(history.history['loss'], label='Training')\n",
        "plt.plot(history.history['val_loss'], label='Validation')\n",
        "plt.title('ReLU Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t9z9380x2ToR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of single-layer RNN Model with ReLU activation in Hidden Layers\n",
        "\n",
        "X_test_dropped_rnn = X_test_dropped.values.reshape((X_test_dropped.shape[0], 1, X_test_dropped.shape[1]))\n",
        "X_test_dropped_rnn = tf.convert_to_tensor(X_test_dropped_rnn, dtype=tf.float32)\n",
        "y_test_rnn = tf.convert_to_tensor(y_test.values, dtype=tf.float32)\n",
        "\n",
        "loss, accuracy, auc = model_relu.evaluate(X_test_dropped_rnn, y_test_rnn, batch_size=32)\n",
        "\n",
        "y_pred_rnn = model_relu.predict(X_test_dropped_rnn)\n",
        "\n",
        "threshold = 0.5\n",
        "predicted = tf.cast(y_pred_rnn >= threshold, tf.float32).numpy()\n",
        "\n",
        "y_test_rnn = y_test_rnn.numpy()\n",
        "\n",
        "confuse_matrix = confusion_matrix(y_test_rnn, predicted)\n",
        "recall = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[1,0])\n",
        "precision = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[0,1])\n",
        "f1 = (2*precision*recall)/(precision + recall)\n",
        "\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}, Test AUC: {auc}')\n",
        "print(f'Confusion Matrix : {confuse_matrix}')\n",
        "print(f'Precision:{precision}')\n",
        "print(f'Recall:{recall}')\n",
        "print(f'F1: {f1}')"
      ],
      "metadata": {
        "id": "QPdDbfPr2XAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and Training RNN Model with Leaky ReLU activation in Hidden Layers\n",
        "import tensorflow as tf\n",
        "\n",
        "model_leaky = tf.keras.Sequential([tf.keras.layers.SimpleRNN(units=128, activation=None, input_shape=(1, X_train_resampled.shape[1])),\n",
        "                             tf.keras.layers.LeakyReLU(alpha=0.01),\n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model_leaky.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
        "\n",
        "X_train_resampled_rnn = X_train_resampled.values.reshape((X_train_resampled.shape[0], 1, X_train_resampled.shape[1]))\n",
        "X_train_resampled_rnn = tf.convert_to_tensor(X_train_resampled_rnn, dtype=tf.float32)\n",
        "y_train_resampled_rnn = tf.convert_to_tensor(y_train_resampled.values, dtype=tf.float32)\n",
        "\n",
        "X_val_resampled_rnn = X_valid_dropped.values.reshape((X_valid_dropped.shape[0], 1, X_valid_dropped.shape[1]))\n",
        "X_val_resampled_rnn = tf.convert_to_tensor(X_val_resampled_rnn, dtype=tf.float32)\n",
        "y_val_resampled_rnn = tf.convert_to_tensor(y_valid.values, dtype=tf.float32)\n",
        "\n",
        "eval_set = (X_val_resampled_rnn, y_val_resampled_rnn)\n",
        "\n",
        "history_leaky = model_leaky.fit(X_train_resampled_rnn, y_train_resampled_rnn, epochs=15, batch_size=32, validation_data=eval_set)"
      ],
      "metadata": {
        "id": "EFQtnzmL2cpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot binary cross entropy loss\n",
        "\n",
        "plt.plot(history_leaky.history['loss'], label='Training')\n",
        "plt.plot(history_leaky.history['val_loss'], label='Validation')\n",
        "plt.title('Leaky ReLU Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ImEp74cL3zwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of single-layer RNN Model with Leaky ReLU activation in hidden layers\n",
        "\n",
        "X_test_dropped_rnn = X_test_dropped.values.reshape((X_test_dropped.shape[0], 1, X_test_dropped.shape[1]))\n",
        "X_test_dropped_rnn = tf.convert_to_tensor(X_test_dropped_rnn, dtype=tf.float32)\n",
        "y_test_rnn = tf.convert_to_tensor(y_test.values, dtype=tf.float32)\n",
        "\n",
        "loss, accuracy, auc = model_leaky.evaluate(X_test_dropped_rnn, y_test_rnn, batch_size=32)\n",
        "\n",
        "y_pred_rnn = model_leaky.predict(X_test_dropped_rnn)\n",
        "\n",
        "threshold = 0.5\n",
        "predicted = tf.cast(y_pred_rnn > threshold, tf.float32).numpy()\n",
        "\n",
        "y_test_rnn = y_test_rnn.numpy()\n",
        "\n",
        "confuse_matrix = confusion_matrix(y_test_rnn, predicted)\n",
        "recall = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[1,0])\n",
        "precision = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[0,1])\n",
        "f1 = (2*precision*recall)/(precision + recall)\n",
        "\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}, Test AUC: {auc}')\n",
        "print(f'Confusion Matrix : {confuse_matrix}')\n",
        "print(f'Precision:{precision}')\n",
        "print(f'Recall:{recall}')\n",
        "print(f'F1: {f1}')"
      ],
      "metadata": {
        "id": "POcckerZ31Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models with LSTM Layers"
      ],
      "metadata": {
        "id": "0k6Vo4294A5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and Training RNN Model with LSTM Hidden Layers\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "model_LSTM_relu = tf.keras.Sequential([tf.keras.layers.LSTM(units=128, activation='relu', input_shape=(1, X_train_resampled.shape[1])),\n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model_LSTM_relu.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
        "\n",
        "X_train_resampled_lstm = X_train_resampled.values.reshape((X_train_resampled.shape[0], 1, X_train_resampled.shape[1]))\n",
        "X_train_resampled_lstm = tf.convert_to_tensor(X_train_resampled_rnn, dtype=tf.float32)\n",
        "y_train_resampled_lstm = tf.convert_to_tensor(y_train_resampled.values, dtype=tf.float32)\n",
        "\n",
        "X_val_resampled_rnn = X_valid_dropped.values.reshape((X_valid_dropped.shape[0], 1, X_valid_dropped.shape[1]))\n",
        "X_val_resampled_rnn = tf.convert_to_tensor(X_val_resampled_rnn, dtype=tf.float32)\n",
        "y_val_resampled_rnn = tf.convert_to_tensor(y_valid.values, dtype=tf.float32)\n",
        "\n",
        "eval_set = (X_val_resampled_rnn, y_val_resampled_rnn)\n",
        "\n",
        "history_lstm = model_LSTM_relu.fit(X_train_resampled_lstm, y_train_resampled_lstm, epochs=15, batch_size=32, validation_data=eval_set)"
      ],
      "metadata": {
        "id": "x44WpYa14AcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot binary cross entropy loss\n",
        "\n",
        "plt.plot(history_lstm.history['loss'], label='Training')\n",
        "plt.plot(history_lstm.history['val_loss'], label='Validation')\n",
        "plt.title('LSTM Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rKKMSByI6iHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of RNN Model with LSTM Hidden Layers on Test Set\n",
        "\n",
        "X_test_dropped_rnn = X_test_dropped.values.reshape((X_test_dropped.shape[0], 1, X_test_dropped.shape[1]))\n",
        "X_test_dropped_rnn = tf.convert_to_tensor(X_test_dropped_rnn, dtype=tf.float32)\n",
        "y_test_rnn = tf.convert_to_tensor(y_test.values, dtype=tf.float32)\n",
        "\n",
        "loss, accuracy, auc = model_LSTM_relu.evaluate(X_test_dropped_rnn, y_test_rnn, batch_size=32)\n",
        "\n",
        "y_pred_rnn = model_LSTM_relu.predict(X_test_dropped_rnn)\n",
        "\n",
        "threshold = 0.5\n",
        "predicted = tf.cast(y_pred_rnn > threshold, tf.float32).numpy()\n",
        "\n",
        "y_test_rnn = y_test_rnn.numpy()\n",
        "\n",
        "confuse_matrix = confusion_matrix(y_test_rnn, predicted)\n",
        "recall = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[1,0])\n",
        "precision = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[0,1])\n",
        "f1 = (2*precision*recall)/(precision + recall)\n",
        "\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}, Test AUC: {auc}')\n",
        "print(f'Confusion Matrix : {confuse_matrix}')\n",
        "print(f'Precision:{precision}')\n",
        "print(f'Recall:{recall}')\n",
        "print(f'F1: {f1}')"
      ],
      "metadata": {
        "id": "BWlYiY_N6pDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increased Model Complexity"
      ],
      "metadata": {
        "id": "5MYjKca7-2_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and Training RNN Model with 2 LSTM and 1 Dense Hidden Layers\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "X_train_resampled_lstm = X_train_resampled.values.reshape((X_train_resampled.shape[0], 1, 300))\n",
        "X_train_resampled_lstm = tf.convert_to_tensor(X_train_resampled_lstm, dtype=tf.float32)\n",
        "y_train_resampled_lstm = tf.convert_to_tensor(y_train_resampled.values, dtype=tf.float32)\n",
        "\n",
        "model_LSTM_complex = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1, X_train_resampled_lstm.shape[2])),\n",
        "    tf.keras.layers.LSTM(units=256, activation='relu', return_sequences=True),\n",
        "    tf.keras.layers.LSTM(units=256, activation='relu'),\n",
        "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model_LSTM_complex.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'AUC'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "X_val_resampled_rnn = X_valid_dropped.values.reshape((X_valid_dropped.shape[0], 1, X_valid_dropped.shape[1]))\n",
        "X_val_resampled_rnn = tf.convert_to_tensor(X_val_resampled_rnn, dtype=tf.float32)\n",
        "y_val_resampled_rnn = tf.convert_to_tensor(y_valid.values, dtype=tf.float32)\n",
        "\n",
        "eval_set = (X_val_resampled_rnn, y_val_resampled_rnn)\n",
        "\n",
        "history_complex = model_LSTM_complex.fit(X_train_resampled_lstm, y_train_resampled_lstm, epochs=15, batch_size=32, validation_data=eval_set)"
      ],
      "metadata": {
        "id": "24w6mW6C6uQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot binary cross entropy loss\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history_complex.history['loss'], label='Training')\n",
        "plt.plot(history_complex.history['val_loss'], label='Validation')\n",
        "plt.title('More Complex LSTM Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rlMsS8HJ_GJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of RNN Model with 2 LSTM Hidden Layers and 1 Hidden Dense Layer on Test Set\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "X_test_dropped_rnn = X_test_dropped.values.reshape((X_test_dropped.shape[0], 1, X_test_dropped.shape[1]))\n",
        "X_test_dropped_rnn = tf.convert_to_tensor(X_test_dropped_rnn, dtype=tf.float32)\n",
        "y_test_rnn = tf.convert_to_tensor(y_test.values, dtype=tf.float32)\n",
        "\n",
        "loss, accuracy, auc = model_LSTM_complex.evaluate(X_test_dropped_rnn, y_test_rnn, batch_size=32)\n",
        "\n",
        "y_pred_rnn = model_LSTM_complex.predict(X_test_dropped_rnn)\n",
        "\n",
        "threshold = 0.5\n",
        "predicted = tf.cast(y_pred_rnn > threshold, tf.float32).numpy()\n",
        "\n",
        "y_test_rnn = y_test_rnn.numpy()\n",
        "\n",
        "confuse_matrix = confusion_matrix(y_test_rnn, predicted)\n",
        "recall = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[1,0])\n",
        "precision = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[0,1])\n",
        "f1 = (2*precision*recall)/(precision + recall)\n",
        "\n",
        "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}, Test AUC: {auc}')\n",
        "print(f'Confusion Matrix : {confuse_matrix}')\n",
        "print(f'Precision:{precision}')\n",
        "print(f'Recall:{recall}')\n",
        "print(f'F1: {f1}')"
      ],
      "metadata": {
        "id": "13qNhS3J-_0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting and Training RNN Model with 2 LSTM and 1 Dense Hidden Layers\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "X_train_resampled_lstm = X_train_resampled.values.reshape((X_train_resampled.shape[0], 1, 300))\n",
        "X_train_resampled_lstm = tf.convert_to_tensor(X_train_resampled_lstm, dtype=tf.float32)\n",
        "y_train_resampled_lstm = tf.convert_to_tensor(y_train_resampled.values, dtype=tf.float32)\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='AUC', patience=5, restore_best_weights=True)\n",
        "\n",
        "model_LSTM_complex_reg = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1, X_train_resampled_lstm.shape[2])),\n",
        "    tf.keras.layers.LSTM(units=256,\n",
        "                         activation='relu',\n",
        "                         dropout=0.05,\n",
        "                         recurrent_dropout=0.05,\n",
        "                         kernel_regularizer=tf.keras.regularizers.l2(0.0001),\n",
        "                         return_sequences=True),\n",
        "    tf.keras.layers.LSTM(units=256,\n",
        "                         activation='relu',\n",
        "                         dropout=0.05,\n",
        "                         recurrent_dropout=0.05,\n",
        "                         kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
        "    tf.keras.layers.Dense(units=256,\n",
        "                          activation='relu',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "adam_optimizer = Adam(learning_rate=0.00001)\n",
        "\n",
        "model_LSTM_complex_reg.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
        "\n",
        "history_reg = model_LSTM_complex_reg.fit(X_train_resampled_lstm, y_train_resampled_lstm, epochs=40, batch_size=32,callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "susJwGYs_Ouw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparamter tuning for multi-layer LSTM Model on Validation Set\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "X_valid_dropped_rnn = X_valid_dropped.values.reshape((X_valid_dropped.shape[0], 1, X_valid_dropped.shape[1]))\n",
        "X_valid_dropped_rnn = tf.convert_to_tensor(X_valid_dropped_rnn, dtype=tf.float32)\n",
        "y_valid_rnn = tf.convert_to_tensor(y_valid.values, dtype=tf.float32)\n",
        "\n",
        "loss, auc = model_LSTM_complex_reg.evaluate(X_valid_dropped_rnn, y_test_rnn, batch_size=32)\n",
        "\n",
        "threshold = 0.5\n",
        "y_pred_rnn = model_LSTM_complex_reg.predict(X_test_dropped_rnn)\n",
        "predicted = tf.cast(y_pred_rnn > threshold, tf.float32).numpy()\n",
        "\n",
        "y_test_rnn = y_test_rnn.numpy()\n",
        "\n",
        "confuse_matrix = confusion_matrix(y_test_rnn, predicted)\n",
        "recall = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[1,0])\n",
        "precision = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[0,1])\n",
        "f1 = (2*precision*recall)/(precision + recall)\n",
        "\n",
        "print(f'Validation Loss: {loss}, Validation AUC: {auc}')\n",
        "print(f'Confusion Matrix : {confuse_matrix}')\n",
        "print(f'Precision:{precision}')\n",
        "print(f'Recall:{recall}')\n",
        "print(f'F1: {f1}')"
      ],
      "metadata": {
        "id": "0fiATkcQ_y02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation of RNN Model with 2 LSTM Hidden Layers and 1 Hidden Dense Layer (along with regularization, early stopping, and dropout) on Test Set\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "X_test_dropped_rnn = X_test_dropped.values.reshape((X_test_dropped.shape[0], 1, X_test_dropped.shape[1]))\n",
        "X_test_dropped_rnn = tf.convert_to_tensor(X_test_dropped_rnn, dtype=tf.float32)\n",
        "y_test_rnn = tf.convert_to_tensor(y_test.values, dtype=tf.float32)\n",
        "\n",
        "loss, auc = model_LSTM_complex_reg.evaluate(X_test_dropped_rnn, y_test_rnn, batch_size=32)\n",
        "\n",
        "threshold = 0.5\n",
        "y_pred_rnn = model_LSTM_complex_reg.predict(X_test_dropped_rnn)\n",
        "predicted = tf.cast(y_pred_rnn > threshold, tf.float32).numpy()\n",
        "\n",
        "y_test_rnn = y_test_rnn.numpy()\n",
        "\n",
        "confuse_matrix = confusion_matrix(y_test_rnn, predicted)\n",
        "recall = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[1,0])\n",
        "precision = confuse_matrix[1,1]/(confuse_matrix[1,1] + confuse_matrix[0,1])\n",
        "f1 = (2*precision*recall)/(precision + recall)\n",
        "\n",
        "print(f'Test Loss: {loss}, Test AUC: {auc}')\n",
        "print(f'Confusion Matrix : {confuse_matrix}')\n",
        "print(f'Precision:{precision}')\n",
        "print(f'Recall:{recall}')\n",
        "print(f'F1: {f1}')"
      ],
      "metadata": {
        "id": "QvlJ9RTQ_pPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC curve\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "false_positive, true_positive, _ = roc_curve(y_test_rnn, y_pred_rnn)\n",
        "\n",
        "plt.figure(figsize=(12,9))\n",
        "plt.plot(false_positive, true_positive, color='darkorange', lw=2, label='ROC curve')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tPQHm-olBaFc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}